Setting hostname - sudo hostnamectl set-hostname <new_hostname>
Github push repo to github


1. Yum install git -y
2. ssh-keygen
3. Cd .ssh
4. Copy the public key, now add this in github
5. Cd data/
6. git init
7. git add .
8. git commit -m "Initial commit"
9. git remote add origin git@github.com:arkaprovolti/first-app.git
10. git remote -v
11. git push  origin master
12. 

Github: cloning same repo and pushing it to same repo;
1. git clone git@github.com:arkaprovolti/first-app.git
2. cat > sds.txt
sdaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa yeyyy
3. git add .
4.  git commit -m "Initial commit"
5. git push  origin main




Github- cloning a different repo and pushing it to my repo
1. git clone git@github.com:sanjayguruji/java-code-with-maven.git
2. Now go inside /java-code-with-maven directory cloned
3. git remote -v
4. git remote set-url origin git@github.com:arkaprovolti/java-maven-arka-2.git
5. Git push origin main


Jenkins
1. 15 gb gp2 instance
2.  yum update -y
3. Google search installing jenkins on aws
4. sudo wget -O /etc/yum.repos.d/jenkins.repo \
https://pkg.jenkins.io/redhat-stable/jenkins.repo
5. Yum repolist all
6. sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io-2023.key
sudo yum upgrade
7. Instance -> security groups -> select your one -> edit inbound rules ->add custom tcp -> port range 8080 -> 0.0.0.0/0
8. Now copy public ip : 8080
9. sudo yum install java-17-amazon-corretto -y
10. Sudo yum install jenkins -y
11. Systemctl enable jenkins
12. Systemctl start jenkins
13. Open public ip of server
14. Paste vim  /var/lib/jenkins/secrets/initialAdminPassword and unlock jenkins
15. Now gotta increase size of temp
16. sudo mount -o remount, size=2G /tmp
17. df -h
18. vim /etc/fstab
19. tmpfs /tmp tmpfs defaults,noatime,mode=1777,size=2G 0 0
20. Sudo mount -a
21. Systemctl daemon-reload
22. Reboot instance
23. in case if ec2 instance is shutdown, jenkins node will go slow. To solve this, go to
Cd /var/lib/jenkins -> jenkins.model…….xml -> open it in vim -> we got an ip address, replace it with port ip.
24. Now we gotta integrate jenkins server with our github repo.
25. Add webhooks in our github repo
26. Payload url should look like http://51.20.118.218:8080/github-webhook/
27. You gotta get secret, for this go to jenkins -> security -> create api token, githooks -> allow on agents.
28. Now we will make a pipeline in jenkins -> new item -> freestyle ->ok
29. Go to configure, of the created pipeline, and paste repo url in htpps
30. yum install git
31. Now build pipeline


Tomcat server setup
   1. yum install java -y
yum install wget
wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.108/bin/apache-tomcat-9.0.108.tar.gz
tar -xvzf apache-tomcat-10.1.44.tar.gz
mv apache-tomcat-10.1.44.tar.gz /tmp
 cd apache-tomcat-10.1.44
cd bin/
chmod +x startup.sh
chmod +x shutdown.sh
cd ..
cd apache-tomcat-10.1.44
find -name context.xml -> 3 files be created -> now we gotta last 3 run each of em in vim
vim ./webapps/examples/META-INF/context.xml
cd conf
vim tomcat-users.xml
push given lines between --> and </tomcat-users>: 
    <role rolename="manager-gui"/>
<role rolename="manager-script"/>
<role rolename="manager-jmx"/>
<role rolename="manager-status"/>   
<user username="admin" password="admin" roles="manager-gui,manager-script,manager-jmx,manager-status"/>
<user username="deployer" password="deployer" roles="manager-script"/>
<user username="tomcat" password="s3cret" roles="manager-gui"/>

cd bin
./startup.sh

Instance -> security groups -> select your one -> edit inbound rules ->add custom tcp -> port range 8080 -> 0.0.0.0/0
Now copy public ip : 8080





Integrating java GitHub repo with Jenkins (after Jenkins server is setup)

yum install git -y
yum install maven -y
mvn -v  --> can see maven hom path and java path
Now you gotta install plugins - maven integration, deploy to container, GitHub integration
Now restart Jenkins
Now you gotta path set -> Manage Jenkins -> Tools -> add jdk ->java path -> maven- maven path
Now we gotta integrate jenkins server with our github repo.
26. Payload url should look like http://51.20.118.218:8080/github-webhook/
27. You gotta get secret, for this go to jenkins -> security -> create api token, githooks -> allow on agents.
Now new build -> maven project -> git repo https paste -> untick snapshot -> apply and save












Launching index.html on httpd server

1. Launch an ec2 instance
yum install httpd -y
rpmquery httpd
cd /var/www/html
 cat > index.html
give port no.80 
systemctl start httpd
systemctl enable httpd
http://54.235.42.184/:80


Making AMI of instance
1. go to your instance
actions -> image ->create image.
the ami will be created in ami catalog -> my ami
Now launch instance with AMI
Create instance
You will see the same machine content will be copied into a different machine
Now you want to share this AMI for a different account, so go to AMI -> Shared account id ->add account id
Now that account id can see your AMI into his, he can make instance of that and the files will be shared.









Creating a template and launching 2 instances

Ec2->Launch templates -> Create new template -> My AMIS -> owned by me ->  EBS Volume 10gb gp2
Go to a instance -> Launch instance from template
Now I want script automatically triggered -> Advanced Settings -> User Data ->
	yum install httpd -y
	cat > index.html
	systemctl start httpd
	systemctl enable httpd
	useradd John
	passwd John
create launch template
Now, go to an instance and launch instance from template -> give no. of instances 2 -> launch instance
Now we can see 2 instances loaded for the given templat -> You can see if public ip is pasted with port 80 (make sure to configure port 80 in inbound rules)

We can modify template by launching template -> Modify template





Creating Volume in an instance

1. lsblk
2. Now we will create 5 gb volume -> EC2 -> volume -> 5GB
3. Now tick the created volume -> actions -> attach volume -> attach our created instance -> give a device ->
4. lsblk -> we can see our new volume now
5. Now how to access our new disk??
6. lsblk -fs -> we can't see our 5 gb created volume.
7. blkid -> This will show our UUID
mkfs.ext4 /dev/xvdb
8. mkdir /data
9. mount /dev/xvdb /data
df -h
cd /data/
touch Sanjaya.txt{1....100} ->100 blank files created
ll
Now temporary mounting is done. For permanent mounting, we have to follow next steps.
You can get uiid from blkid
vim /etc/fstab -> write
	/dev/xvdb	/data	ext4	defaults 0 0
Restart instance


Extending root volume of an instance

1. Launch an instance
2. create an httpd server
yum install httpd -y
rpmquery httpd
cd /var/www/html
 cat > index.html
give port no.80 
systemctl start httpd
systemctl enable httpd
curl http://localhost -> now the server is live
df -h -> we can see /dev/xvda1 is 10 Gb, which isn't enough for us en, we gotta increase its volume
Now go to volume -> select the volume of the created instance -> modify volume
Now change size to 20 gb
df -h -> but still 10 gb is visible in /dev/xdva1 because OS can read only that storage which have filesystem
So google how to extend xfs filesystem in rhel 8 in aws ec2
growpart /dev/xvda1
growpart /dev/xvda 1
sudo xfs_growfs -d /
df -h -> Now /dev/xvda1 size is increased to 20 gb





Creating a new volume and attaching it with our instance, and also make 2 partitions of this new volume

Create new volume - 7gb, attach it with our instance
fdisk -l
Now, for partitioning we'll use MS DOS Partition scheme
lsblk
fdisk /dev/xvdb
m
n
e
4
 give both sector empty
p
lsblk -> xvdb5 and xvdb6 created we can see
fdisk -l
Now we gotta create mount point. We can use either ext4 or xfs filesystem for xvdb5 and xvdb6 partition.
Now create 2 directories, and mount both in these directories.
mkdir /devops
mkdir /data
mount /dev/xvdb5 /data/
mount /dev/xvdb6 /devops/
cd /devops
touch arka.txt{1...5}

Now if we wanna unmount them and detach them from cloud, and then delete the volume
umount /devops
umount /data





Data replicate in ec2 into different region same zone

1. HW given ->  Data replicate in ec2 into same region different zone
For this we have to use snapshot. It inherits all zones of our region. Snapshot is stored in AWS S3 in backend, but not reachable for us.
2. Our Volume -> create snapshot -> create snapshot
3. Now choose different region and go to snapshot (Ohio - us-east-1b) -> not reachable. Because Snapshot is a region based service. Now copy snapshot and choose us-east-2.




Data replicate in ec2 into same region different zone

For this we have to use snapshot. It inherits all zones of our region. Snapshot is stored in AWS S3 in backend, but not reachable for us.
2. Our Volume -> create snapshot -> create snapshot
3. Now for the created snapshot -> we create volume from snapshot and choose a different region.
Now create an instance for that volume


Data replication in EFS by 3 different machines in 3 different zones in same region

1. Create 3 instances 8 gb each (amazon Linux, redhat, ubuntu) in 3 different regions
2. Same security group with all 3 machines
3. Search EFS -> create file system 
4. For the file system created -> view ->edit
We gotta install nfs in all of 3 machines (nfs is by default installed in amazon Linux)
amazon machine -> rpmquery nfs-utils
		systemctl start nfs-server.service
		systemctl enable nfs-server.service
		systemctl status
redhat -> 
	yum install nfs-utils -y
	systemctl start nfs-server.service
		systemctl enable nfs-server.service
		systemctl status

ubuntu ->
	apt update -y
	apt install nfs-common
	systemctl start nfs-utils.service
	systemctl enable nfs-utils.service
we went to security group and added nfs.
Now go EFS -> file systems -> network -> managet -> security group changed to nfs - a,b,c keep ; deleted rest.
Now we go to our filesystem -> Attach -> Mount via DNS -> copy "using the NFS client" ->
sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport fs-034ce5eaefe890aad.efs.us-east-1.amazonaws.com:/ efs

now we create a directory (data) on either machine in root (for easy), paste the copied sudo mount... and replace efs with our directory.


Now we create another directory in another machine, and paste sudo mount ... and replace efs with our directory, create files in there -> touch fk.txt{1..10}

Now, in our previous mounted machine, in the mounted directory, we can find our files created in previous machine.










Creating a S3 bucket and access it from Amazon Linux instance (and also creating files in machine which shall be reflected in s3 bucket)


s3-> Create bucket _> bucket name, ACLs diabled ->unlock ->acknowledge -> create bucket
Now upload some data on bucket
Now you can't access object with object link.
in permission -> ACl was disabled. So gotta give permission for bucket
Go to -> edit object owbership -> ACL enable
Now u can see for each object -> permission -> Edit is enabled
Now you can edit and give permsiion to objects.
But if deletion of object happens by mistake, u can retain it only if bucket versioning is enabled

1. After bucket is created and files are uploaded, create an instance
2. yum update -y
Now we gotta install some plugins. 
 yum install automake fuse fuse-devel gcc-c++ libcurl-devel libxml2-devel make openssl-devel
yum install git
git clone https://github.com/s3fs-fuse/s3fs-fuse.git
now move to s3fs-fuse directory
./autogen.sh
./configure --prefix=/usr --with-openssl
make
make install
which s3fs
Package is installed, but s3 bucket and instance have different ip so how to access em
AWS ->iam-> create user -> attach policies directly -> give full acess policy rights -> create user
Go to security credential -> create access key -> CLI -> download csv
Now go to cli
touch /etc/passwd-s3fs
vim /etc/passwd-s3fs-> give access-key:secret-key
Now change permission of file -
sudo chmod 640 /etc/passwd-s3fs
Now mount
s3fs arka-bucket1231 /mnt -o passwd_file=/etc/passwd-s3fs
df -h -> s3fs
Now goto mount directory 
 cd /mnt/
touch devops.txt{1..5}
This will be reflected in bucket








Creating a S3 bucket and access it from Amazon Linux instance (using AWS CLI)

1. Create S3 bucket
2. Create an amazon Linux instance
yum install unzip -y
rpmquery mount-s3 -> mount-s3 package not installed, so we gotta install that.
yum install wget -y
wget https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm
yum install ./mount-s3.rpm
Now we need authentication for different services (EC2/local machine and S3 bucket ) to communiccate
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscli2.zip"
unzip awscli2.zip
aws configure -> share access key and secret key from csv -> us-east-1 -> o/p= table
sudo ./aws/install
cd .aws
cat config
cat credentials
Now we have to create a mountpoint, so we create a directory.
mkdir /pooja
mount-s3 arka-bucket1231 /pooja/
Now to go ACL -> give read write permissions and authorize.
cd pooja -> You can see your bucket files.


S3 bucket access from windows instance

1. Create t3.medium instance -> windows machine 2019 base 
security group -> rdp protocol allowed for windows
30 gb gp2.
2. Download RDP file -> open -> connect -> Asking for password
3. Get password in the instance only - upload password
4. Go to server manager -> open -> local server -> ie  -> of of enhance
5. Download WIndows Tnt drive -> install trial .
6. Open tnt drive -> add new drive -> add -> any account name -> access key and password from csv file for authentication -> add new account -> save changes
7. S3 bucket searched -> add new drive -> open it -> create new file -> got out files from s3 bucket.




Static website hosting from s3 bucket

1. open s3 bucket (empty objects).
2. Add all files of any web project (html,css,js)
3. Go to properties -> static web hosting -> edit -> enable ->host static website -> index.html
4. Bucket website endpoint url -> click
5. Permission -> ACL ->edit -> Read, write
6. Seelct all objects -> Action -> make it public -> we got our website


















Creating a load balancer (Create 4 instances on different zones of same region, and while one server is down, other servers would work)

1. Create 4 instances on diff zones.
2. httpd server on all 4 serving index.html
Firstly create target group -> instances -> register targets -> add all machines -> create target group (Ensure all machines are added to the target group)

3. We have to create cross zone load balancer to ensure that if 1 zone gets down, not all can go down.
4. For httpd server, we have to create application load balancer
5. Now how would this load balancer distribute load in the network?? Lets see
6. go to load balancer -> internet facing -> zone a,b,c because our servers in a,b,c -> security group 80 port allowed -> start and enable httpd in all 3 machines -> copy dns and paste




















Creating auto scaling

Autoscaling 

1. Create template -> Browse AMI -> select amazon Linux -> give zone 1a -> create
2. view launch templates
select launched instance -> create auto scaling group ->next
Choose instance launch options -> Instance type requirements -> specify instance attributes -> vpcu 1min 2max -> memory 1min 4max -> Include On-Demand base capacity -> 2 -> in availability zones -> 1a,1b,1c -> Balanced Best Efforts ->next
Integrate with other services-> add new load balancer -> internet facing -> no vpc -> next
Configure group size and scaling -> desired capacity =2 -> scaling 2min 6max ->automatic scaling = target cpu scaling -> avg. cpu utilization -> target value=50, instance warmup 180 secs ->  instance custom policy - custom ->next
Create Auto scaling group
Now in instance can see 2 instances running -> one in us east 1a and other in us east 1b
Now in both instances run this command -> yes > /dev/null & -> This runs yes command in the background, continuously 	generating"y" output. This consumes CPU resources until the process is terminated.
Run command - top
The top command hsows cpu usage, memory usage, process list
Remember the process id of both the machines
Now load will be increased and multiple instances will be created to distribute the load
Now, if we kill either machine or both, the load is reduced, and extra machins will be terminated.
Command to kill process -> kill <PID>







Create VPC and then access EC2. 

1. We have to follow the rule VISR
	V -> VPC ->CIDR (19.0.0/60)
	I -> IGW
	S -> Subnet
 	R -> Route Table.
2. For real life analogy, consider floors as subnets, building as VPC, and every room as EC2.
	So, 1 subnet can have many EC2.
3. We will create 2 subents - private and public. Private won't be exposed over internet, eg. Database. Public will be exposed over internet, eg. Web server, 	Mail server.
4. Suppose, for 2 subnets we wanna create, we allot 250 IPS for one, and 150 IPS for the other. Now how will we subnet? For that we can use vlsm calculator.
5. Now go to AWS -> Create VPC -> VPC only ->IPV4 CIDR -> put 10.0.0.0/16 -> no ipv6 cidr -> create
Now we can see 2 VPCs. The first one is the default one and we don't have to mess with it.
6. Now we have to create IGW (Internet gateway)
7. We can already see one IGW. Now we gotta create one. Give name :arka-igw
8. Now we gotta create subnet -> give arka-vpc -> give name public-subnet -> give zone 1a ->ipv4 vpc cidr block: 10.0.0.0/16 -> ipv4 subnet cidr block: 10.0.0.0/24 -> create subnet
9. Again we create another subnet -> give name private-subnet -> we give zone 1b -> ipv4 vpc cidr block: 10.0.0.0/16 -> ipv4 subnet cidr block: 10.0.1.0/24 -> create subnet
10. Now we gotta create EC2. 2 EC2s to create -> one for server and another for database, both instances should be in different zones.  For the server EC2, we gotta turn on httpd server.
11. For instance 1-> give our vpc -> our public subnet -> disable auto assign public ip -> security group ->create security group new ->launch instance
12. For instance 2-> same key as instance 1 -> disable auto assign public ip -> new security grp -> our private subnet -> 
13. Now for instance 1, allow icmp -> 0.0.0./0 on its security group.
Then we can see that for our server, the public ip is not visible. So we go to elastic ip
allocate elastic ip -> for that elastic ip -> actions -> associate elastic ip -> give that instance

14. Our IGW is not connected with VPC. So we gotta attach IGW with VPC.
15. Now we gotta create Route table (Don't mess with default one) and attach with vpc
16. Now edit routes ->add route 0.0.0.0/0 -> target: IGW
17. Now route table needs to associate with subnet -> our created route-table -> edit subnet association -> select public subnet -> save associations
18. The server instance should work .
19. Now for instance 2(db), our private subnet -> allow icmp -> 0.0.0./0 on its security group,  disable auto assign public ip -> security group ->create security group new 
20. Now NAT Gateway needed for db instance .
21. Create NAT Gateway -> subnet- public ->connectivity type - public -> create
22. Now gotta create another route table ->our vpc-> create -> edit route-> add route 0.0.0.0/0 -> target -NAT -> 
Now for our created route table -> edit subnet association -> select private subnet -> save
Now from our public server -> sudo su -
vim key14auga.pem (from our public server)
paste the full key
 chmod 400  key14auga.pem
Now try accessing the database server, it will run.
ping google
IT WORKS



H.W -North Virginia - Ohio data communication using VPC








Peering connection VPC between Ohio and North Virginia

1. VISR (for Virginia) vpc- ipv4 10.0.0.0/16, public subnet- 10.0.0.0/24, private subnet - 10.0.1.0/24
 2. VISR (for Virginia) vpc- ipv4 20.0.0.0/16, public subnet- 20.0.0.0/24, private subnet - 20.0.1.0/24
3. instance - auto assign ip enable -> subnet -public
attach both igws (ohio and virgina) to vpc
Now, before peering connection, go to both route tables (ohio virginia) and 0.0.0/0 and select igw.
Now peering connection in both ohio and virginia -> select my account and another region -> select ohio for virgina and ovirginia for ohio
now accept peering from both ohio and virginia
route table ->subnet association ->select public (Do for both ohio and virginia)
Try launching instance at last
After peering connection, in route table, give virginia-ip/16 for ohio and ohio-ip/16 for virginia







Installing docker

	search docker install in rhel 8 in google


yum repolist all
sudo dnf -y install dnf-plugins-core
 sudo dnf config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo
yum repolist all -> now we can see docker- docker ce
sudo dnf install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
sudo dnf install docker-ce*
yum install docker* -y
rpmquery docker
systemctl start docker
systemctl enable docker
docker info

overlay2 -> storage driver
/var/lib/docker -> docker root directory

docker ps -> How many containers running
docker ps -a
docker image ls  -> how many images.
	Now how to get the image? -> DockerHub. No registration needed to pull the image, but registration needed to push the docker image.
	Now, suppose we wanna run ubuntu in container... Then we gotta pull docker image. repo -> collection of images
	Get ubuntu image -> docker pull ubuntu:rolling
	Get httpd image -> docker pull httpd:trixie
	Get centos image -> docker pull centos:centos7.9.2009

So, before deploying container, we get to install these images.
docker image ls -> list of our 3 images installed
docker --help

docker run -it --name <container_name> <image_name> /bin/bash
docker run -it --name arka-devops-container ubuntu:rolling /bin/bash

docker ps -a

apt update -y


docker exec -it 8b8d54855d19 /bin/bash -> get into running docker container (ours is ubuntu container)
apt update -y
apt install apache2 -y
cd /var/www/html
 echo "This is my application one">index.html
systemctl apache2 start -> won't work in our running container
service apache2 start
curl http://localhost -> won't work in our runnin container
apt install curl -> curl is a command line browser
cd
mkdir data
cd data
touch arka.txt{1..10}
exit -> we exited from our container arka-devops-container
docker inspect arka-devops-container | less
ip a s
Now we gonna run another container.....
docker image ls -> ubuntu, httpd, centos

docker run -it --name arka-devops-container2 ubuntu:rolling /bin/bash 
apt update -y
apt install apache2 -y
cd /var/www/html
cat index.html
service apache2 start
Ctrl P + Ctrl Q -> container exit
After container exit, curl http://172.17.0.2 -> This is my application one

docker attach arka-devops-container2 -> get into running docker container

Docker works on port forwarding

Ok, now we want my containers to run on a different port.



docker run -it --name arka-devops-container3 -p 8080:80 ubuntu:rolling /bin/bash --> 8080:80: Maps port 8080 on 	your host to port 80 inside the container.
apt update -y
apt install apache2 -y

docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' arka-devops-container2 -> check ip 	address of this docker container
curl http://172.17.0.3   -> This is my application 2

curl http://172.17.0.4  ->Failed to connect to 172.17.0.4 port 80 after 0 ms: Could not connect to server
			For this port 80, we have to mention port 8080 in the security group










Stopping the existing container - docker stop <container-name or id>
Removing the container - docker rm <container-name or id>
Run a new container with volume mounted at / : docker run -it --name webApp -v my-vol:/ ubuntu:rolling /bin/bash
killing docker container: docker kill <container>
removing container: docker rm <container>


#We cannot directly modify the mount point of an existing container's volume while it is running or stopped,
as Docker does not allow changing the mount configuration after creation



Creating Docker Volume

docker volume create myvol
docker volume ls
docker run -it --name mycontainer -v my-vol:/www/html ubuntu:rolling /bin/bash



Docker volume on container (method 1)
1. docker volume create myvol
2. Access the volume using a temporary container: 
docker run -it --rm -v myvol:/data ubuntu:rolling /bin/bash   -> -v myvol:/data mounts the myvol volume to /data inside the container.
3.ls -l /data
4.echo "test data"> /data/testfile.txt
exit
5. Inspect the volume location on the host (amazon instance)(read only access):
	docker volume inspect myvol
6. Look for Mountpoint field (/var/lib/docker/volumes/myvol/_data).
7. Access it with root priviledges (read only)
	sudo ls -l /var/lib/docker/volumes/myvol/_data
	sudo cat /var/lib/docker/volumes/myvol/_data/testfile.txt




Docker volume on container(method 2):

1. Create files on machine : mkdir -p /home/user/my-files
2. Add some files to it:
	echo "File from machine 1"> /home/user/my-files/machone-file1.txt
	echo "File from machine 2"> /home/user/my-files/machone-file2.txt
3. Run a container with a bind mount
	docker run -d --name myContainer -v /home/user/my-files:/data ubuntu:rolling -> Binds the host directory to /data in the container.
4. Access the files in container and add files :
	docker exec -it myContainer /bin/bash
5. Create new files in /data
	echo "File1 from container"> /home/user/my-files/container-file1.txt
	echo "File2 from container"> /home/user/my-files/container-file2.txt
6. Remove the container
	docker stop file-test
	docker rm file-test -> This deletes the container, but the bind-mounted data remains on the host (/home/user/my-files)
7. Access the Volume data on the machine
	ls -l /home/user/my-file










Configuring KOPS: (details is on Sanjaya sir's repo)

create instance ubuntu 2nd in the list t2.medium 10gb
apt update -y
install AWSCLI -> 
apt install unzip -y
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

Install kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
kubectl version --client


INstall kops
curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
chmod +x ./kops
sudo mv ./kops /usr/local/bin/


Create an IAM user firstly with AdministratorAccess, Route53full, EC2full, IAMfull and S3full full access -> arka-kops-2
Now create access key -> CLI -> 
Now gotta create role-> Now we create role, after user -> aws service -> ec2 -> Route53full, EC2full, IAMfull and S3full full access -> role name : arka2-kops-role-2 -> go to instance and modify iam role

Now, aws configure

Now, Install kops on ubuntu instance:
 curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64

 chmod +x kops-linux-amd64
 sudo mv kops-linux-amd64 /usr/local/bin/kops
kops version 

Create a Route53 private hosted zone (you can create Public hosted zone if you have a domain) ->
Route 53 ->Hosted zones -> create hosted zones -> now while giving name give it in Sanjaya sir's format to be on the safe side -> name: k8s.arka-2-melo.in -> private hosted zone ->

create an S3 bucket: (To be on safe side, give name like Sanjaya Sir's one (dev.k8....)(Give dns name same as bucket)
aws s3 mb s3://dev.k8s.arka-2-melo.in -> make_bucket: dev.k8s.arka-2-melo.in

To Check Bucket: aws s3 ls


Expose environment variable:

 export KOPS_STATE_STORE=s3://dev.k8s.arka-2-melo.in
Create sshkeys before creating cluster

 ssh-keygen

Create kubernetes cluster definitions on S3 bucket: 

kops create cluster --cloud=aws --zones=us-east-1a --name=dev.k8s.arka-2-melo.in --dns-zone=k8s.arka-2-melo.in --dns private

Create kubernetes cluser

  kops update cluster dev.k8s.arka-2-melo.in --yes --admin


Now worker node and master node will be created in instance, wait for 10 mins, they will be ready.

To list nodes
	kubectl get nodes 

Validate your cluster
	kops validate cluster










Kubernetes cluster configuration in master, worker1 and worker 2 (Sanjaya-K8S-Code
/k8s-installtion-with-containerd-on-ubuntu 24.04)



1. ICMP allowed in all 3, with separate for master.
2. t2.medium for master while t2.micro for workers.
3. ip a s -> note down ip address of all 3 machines and at vim.etc/hosts, paste 3 ip with names at all 3.	
	172.31.22.148   master
	172.31.20.241   worker1
	172.31.26.74    worker2
4. try pinging from all 3 to all 3.
5. Now try sir's GitHub repo: k8s-installtion-with-containerd-on-ubuntu 24.04
for master:
	run codes till  sudo chown.
In this command -> vim /etc/containerd/config.toml-> line 139 (:se nu) -> make it true
6. for workers:
	run codes till sudo systemctl enable --now kubelet..
	dont run kubeadm pull and init.
7. then in workers run: 
kubeadm join 172.31.31.6:6443 --token mf0okw.911w0h06l3gffq1a \
        --discovery-token-ca-cert-hash sha256:815d91ea8044ddc548deb6ef3a5890f2ff965b78db4322aaab4de9435a44f89c

8. Then allow ports: 
	port 6443 should be allowed
	for master:
	https://kubernetes.io/docs/reference/networking/ports-and-protocols/

	for workers:
	https://kubernetes.io/docs/reference/networking/ports-and-protocols/

Then:
Configure Pod Network
curl https://raw.githubusercontent.com/projectcalico/calico/v3.29.2/manifests/calico.yaml -O

kubectl apply -f calico.yaml








For ubuntu, the worker node setup is same asa given before, foor redhat9, try following the Sanjaya-K8S-Code
/Host K8S cluster using Docker with RHEL-9, documentation, gotta install some packages if some not working, also firewalld gotta install and enable, and exclude the kubelet command in the ery last for it to work.

But still, that node won't be ready and for that we have to follow these steps to make it ready in that worker node:

sudo mkdir -p /run/systemd/resolve
sudo ln -s /etc/resolv.conf /run/systemd/resolve/resolv.conf
ls -l /run/systemd/resolve/resolv.conf
sudo systemctl restart kubelet






Kubernetes replicaset

1. After cluster is setup, with 3 ubuntu , and one redhat 9, run these.
2. kubectl get pod -> No resources found in namespace
3. kubectl get pod --all-namespaces -> all pods.
4. kubectl get namespaces -> all namespaces
5. system pods -> kubectl get pod -n kube-system
6. 
kubectl get nodes 

Creating a Pod with Apache in the default namespace
vim apache-pod-default.yaml
1. Create a yaml file named apache-pod-default.yaml to define the pod.
apiVersion: v1
kind: Pod                                                                                                                            metadata:                                                                                                                              name: apache-pod                                                                                                                     namespace: default                                                                                                                 spec:                                                                                                                                  containers:
  - name: apache-container                                                                                                               image: httpd-latest                                                                                                                  ports:                                                                                                                               - containerPort: 80   

In the above Yaml file, a pod named apache-pod in the default namespace, running an Apache container using the httpd:latest image, exposing port 80.

Create the pod > kubectl apply -f apache-pod-default.yaml

pod/apache-pod created

kubectl get pod -n default -> Pods in the default namespace

Creating a pod with apache image in the Arka Namespace:
	kubectl create namespace arka
	kubectl get namespaces
	Create a yaml file apache-pod-default.yaml in Arka Namespace.

apiVersion: v1
kind: Pod
metadata:
  name: apache-pod
  namespace: default
spec:
  containers:
  - name: apache-container
    image: httpd-latest
    ports:
    - containerPort: 80


Apply -> kubectl apply -f apache-pod-default.yaml 

kubectl get pod -n arka -> pods in arka namespace















Ansible setup 3 machines

1. Password set in all - passwd root
Follow 2 and 3 in all machine
2. vim /etc/ssh/sshd_config -> password authentication, rootpermitlogin -> yes
3. systemctl restart sshd ->  systemctl enable sshd
4. generate ssh key in master - ssh-keygen
 	If u wanna check ssh key -> cd .ssh/
5. In  master -> ssh-copy-id root@<ip of  machine1>
ssh-copy-id root@<ip of  machine2>
in master -> scp /etc/hosts root@<ip of machine1>:/etc/hosts
scp /etc/hosts root@<ip of machine2>:/etc/hosts
now check in vim etc/hosts in workers
6. now we write in master vim /etc/hosts all ips with names, then scp .... in both machines, then check in both machines vim /etc/hosts -> They are all copied :)
7. NNOW WE GONNA DOWNLOAD ANSIBLE
8. yum install ansible -y
9. check in controller machine in etc/ directory if ansible directory is present or not, if not then make one ansible directory.
10. copy ansible.cfg file from sanjaya sir's github repo ansible-code/ansible.cfg -> vim /etc/ansible/ansible.cfg.
11. in etc/ansible -> create vim hosts -> write
[dev-server]
worker1                                                                                                                                                                                                                                                                   [prod-server]                                                                                                                        worker2                                                                                                                              





Azure setup
1. Search resource groups
2. Create resource groip
3. Now create virtual network -> give previously created resource group
4. Now create virtual machine -> security standard -> subnet -> india-vnet (not india-rg)
5. Now fro same ssh key7, create 3 machines
6. Our office network blocked it, so used phone hotspot instead.
went to virtual machine azure native cli -> gave ssh key copy location -> got our ssh key authentication -> pasted in command prompt.








Ansible mechanism:

in confg file in ansible -> [defaults] host_key_checking = False -> because mine not working
1. ansible all -m user -a "name=Arka uid=1100 home=/devops state=present"  -> This will make devops directory in both worker machines, and Arka user will be created in both machines -> can be verified by cat etc/passwd

2. cat /etc/passwd | grep -i Arka -> run in both hosts ->  To see if the user "Arka" was created on both machines

3.  Now we gonna do industry way

3. go to etc/ansible folder, and then create yaml file -> my-first-playbook.yaml

---
- name: Installing apache and running
  hosts: all

  tasks:
          - name: creating user Jack Sparrow
            user:
                    name: ArkaBoss
                    uid: 1200
                    home: /ltimindtree
                    shell: /bin/bash
                    state: present

          - name: Installed Apache on my remote server
            yum:
                    name: httpd
                    state: present

          - name: started apache service
            systemd:
                     name: httpd
                     state: started
                     enabled: true


If we just wanted to configure a simple apache server:
---
- name: Installing apache and running
  hosts: all
  tasks:
    - name: configure a simple apache server
      yum:
        name: httpd
        state: present

    - name: Started Apache on my remote server
      service:
        name: httpd
        state: started
        enabled: true

    - name: create index.html file
      copy:
        content: "Simple Apache Server."
        dest: /var/www/html/index.html





For checking syntax error -> ansible-playbook my-firstplaybook.yaml --syntax-check
For dry run -> ansible-playbook my-firstplaybook.yaml -C 
For actual run -> ansible-playbook my-firstplaybook.yaml
Then allow port 80 on security group and check with worker node's public ip.




H.W -> EC2 provisioning via ansible











Azure V-net, subnet, resource group and vm setup

1. Create resource grp firstly - lalaland-rg
2. Create v-net ->name=vnet1
3. go to vnet1 -> create subnet -> public subnet -> 20.1.0/24, pvt subnet -> 20.0.2.0 -> Enable private subnet (no default outbound access)
4. Create nsg -> lalaland-nsg
create virtual machine ->server1(public subnet) ->us east ->ssh -> standard security type -> centos -> 
networking ->subnet-public subnet, public ip-new, nic security group - advanced ->     -> select public subnet-> public inbound ports - none -> select our nsg in configure nsg ->

create virtual machine ->server2(private subnet) ->us east ->ssh -> standard security type -> centos -> 
networking ->subnet-public subnet, public ip-new, nic security group - advanced ->     -> select public subnet-> public inbound ports - none -> select our nsg in configure nsg ->

Go to seever1(public) ->networking ->  network settings -> add inbound rules ->  SSH .

In our public server, we have to launch our private server because there is no network in private server (because we turned off the public ip to none).

Now in server1, set password: passwd root -> 
vim 26augkeya.pem -> paste the downloaded key
chmod 400 26augkeya.pem -> for server 2 to access server 1
start the server 2 in server 1 -> ssh -i 26augkeya.pem arka@20.0.2.4





Peering connection b/w tow machines different regions in Azure:

1. Create resource group, vnet (10.0.0.1), nsg in region 1 (US east) (subnet no need to create can use default if cidr is same as vnet), then launch machine in that vnet and region.
2. Create resource group, vnet (20.0.0.1), nsg in region 2 (US west) (subnet no need to create can use default if cidr is same as vnet), then launch machine in that vnet and region.
3. Now make peering connection in vnet1 -> settings -> peering ->add ->select vnet2 
4. Check in vnet2 if peering connection made or not.
Update both nsg rules with icmp.
ping public ip from machine 1 to check connection




















Load Balancer in Azure

1. Create 1 vnet attached with a resource group Create 3 instances with httpd installed and for each nsg instance allow port 80.
2. Run index.html on all 3 machines on port 80
3. Search load balancer -> create lb -> standard lb -> public -> regional -> frontend pool- create pool, create ip -> backend pool (later) ->inbound rules (later) ->  create load balancer
4. Now after load balancer is created, inside our load balancer ->settings -> backend pool -> add -> select our vnet -> NIC -> in ip configuration - add all 3 machines
5. Now go to our load-balancer -> health probe -> add (TCP, 80) ->
6. Now our load balancer -> add load balancing rules -> give front end ip, backend pool, port = 80, backend port=80,
give health probe -> save
7. Now in our load balancer -> frontend ip configuration ->copy ip:80

Our load balancer is made.

Now,if we wanna access all 3 machines using 1 ip, then-
delete public ip from all 3 machines, Go to your load balancer -> inbound NAT Rules -> VM -> ipconfig -> frontend port:22, SSH, backendport=2222, TCP, Now can be accessed using load balancer's IP







Now i have my maven project launched on tomcat. Now gotta configure docker and Jenkins on different machines

1. Launch a machine for docker
2. sudo dnf -y install dnf-plugins-core
sudo dnf config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo
sudo dnf install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
yum install docker* -y
rpmquery docker
systemctl start docker
systemctl enable docker
docker info
Now in Jenkins dashboard -> plugins -> publish over SSH -> restart Jenkins
In docker, Jenkins - Password set  - passwd root
Now go to docker -> vim /etc/ssh/sshd_config  ->permitrootlogion- yes , passwordbasedauthentication - yes
systemctl restart sshd
Now go to jenkins -> vim /etc/ssh/sshd_config  ->permitrootlogion- yes , passwordbasedauthentication - yes
systemctl restart sshd
sshd-keygen in both
Then copy id of both into each other.
Now in Jenkins -> ssh root@<docker ip>
Now after login docker in Jenkins, vim /etc/ssh/sshd_config  ->permitrootlogion- yes , passwordbasedauthentication -yes -> systemctl restart sshd

Now go to Jenkins dashboard -> our pipeline -> configure -> Add post build action ->Send build artifacts over SSH -> (leave it for now, we will see it later) -> source files -> **/*

Now go to system -> publish over SSH -> add SSH server -> name- docker -> hostname - docker ip (public)->username- root ->  give machine username
add SSH server -> name- Jenkins -> hostname - Jenkins ip (public)->username- root ->  give machine username

Now ECR ->create
Now create IAM User -> awsconfigure
Now postr build actions ->send artifacts over ssh -> select Jenkins -> source - / -> exec command -> rsync -avh /var/lib/jenkins/workspace/production-pipeline/* root@172.31.37.151:/opt

Now postr build actions ->send artifacts over ssh -> select docker (we created) -> source - **/*, 
exec command -> cd /opt
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 756842284117.dkr.ecr.us-east-1.amazonaws.com
docker build -t my-ecr-29aug .
docker tag my-ecr-29aug:latest 756842284117.dkr.ecr.us-east-1.amazonaws.com/my-ecr-29aug:latest
docker push 756842284117.dkr.ecr.us-east-1.amazonaws.com/my-ecr-29aug:latest

Build in Jenkins

check opt directory in docker -> files should come.


Now eks-clustering
create role ->iamfullaccess , elasticcontainerregistry, eksclusterpolicy
Modify eks instance in settings -> modify iam role
yum update -y
yum install unzip -y
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
-------------------------------------------
eksctl create cluster --name my-cluster --region us-east-1 --version 1.32 --vpc-public-subnets subnet-0fe75bce722a75495,subnet-0485cd30ed80faa5c --without-nodegroup

Now creating nodegroup (make sure of region and subnet id)

 eksctl create nodegroup \
  --cluster my-cluster \
  --region us-east-1 \
  --name my-node-group \
  --node-ami-family Ubuntu2204 \
  --node-type t2.small \
  --subnet-ids subnet-0fe75bce722a75495,subnet-0485cd30ed80faa5c \
  --nodes 3 \
  --nodes-min 2 \
  --nodes-max 4 \
  --ssh-access \
  --ssh-public-key /root/.ssh/id_rsa.pub

Now set password -> passwd root
vim /etc/ssh/sshd_config -> password authentication, rootpermitlogin -> yes
systemctl restart sshd
systemctl enable sshd

Now go to Jenkins -> manage -> system -> add ssh server -> eks node -> hostname - pvt ip -> username - root -> give password -> test -> apply

Now Jenkins ->pipeline -> configure -> eks -> source- / -> exec -> (Run later)
Go to Docker -> cd /opt/ -> regapp.deploy.yaml and regapp.service.yaml
Copy contents of both file.
Go to eks node -> vim regapp.deploy.yaml and regapp.deploy.yaml -> paste both
in docker -> ssh-copy-id-root@<eks-ip> and in Kubernetes ->ssh-copy-id-root@<Jenkins ip> 

Now go to ecr -> image -> copy url -> paste in eks machine -> regapp.deploy.yaml -> paste in image : url.
Now Jenkins ->pipeline -> configure -> eks -> source- / -> exec -> kubectl apply -f regapp.deploy.yaml
kubectl apply -f regapp.service.yaml

Now build ->
in eks -> kubectl get pod
kubectl get svc -> copy link


We had some error in the dockerfile in project repo, so it failed. So we copied the dockerfile from sir's repo, deleted ecr image, and tried some commits in index.jsp file, the image will be created automatically, and then copy image url in our eks's regapp-deploy.yml file -> then in pipeline configure -> eks -> exec commands-> 
kubectl delete -f regapp-deploy.yml
kubectl apply -f regapp-deploy.yml
kubectl apply -f regapp-sevice.yml

Now build -> copy-link:8080/webapp/ -> our app

Now can commit and will be deployed automatically.









Azure setup Tomcat
1. make Kubernetes cluster from cluster se4rvice (make sure to make node pools)
2. make a vm
3. azure cli
sudo apt-get update
sudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-release

# Add Microsoft's signing key
sudo mkdir -p /etc/apt/keyrings
curl -sL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | sudo tee /etc/apt/keyrings/microsoft.gpg > /dev/null

# Add the Azure CLI repository
echo "deb [arch=amd64 signed-by=/etc/apt/keyrings/microsoft.gpg] https://packages.microsoft.com/repos/azure-cli/ $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/azure-cli.list

# Install Azure CLI
sudo apt-get update
sudo apt-get install azure-cli

4. az --version
5. az aks install-cli
kubectl version --client
az login
az aks get-credentials --resource-group new-rg --name new-cluster-2sep
### Step 5: Write the Deployment YAML File for Tomcat
Create a file named `deployment.yaml` using a text editor (e.g., nano or vim):


Apply the service:
```
kubectl apply -f service.yaml
```

Verify the deployment:
```
kubectl get deployments
```
It should show `tomcat-deployment` with 1/1 ready replicas.

Verify pods:
```
kubectl get pods
```
Wait for the pod to reach "Running" status (may take a minute).

### Step 8: Get the Service Details and External URL
Watch for the external IP assignment (AKS LoadBalancer services provision a public IP):
```
kubectl get svc tomcat-service -w
```

This will output something like:
```
NAME             TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)        AGE
tomcat-service   LoadBalancer   10.0.123.45    <pending>      80:31234/TCP   1m
```
Wait until `EXTERNAL-IP` changes from `<pending>` to an actual IP address (e.g., `20.123.45.67`). Press Ctrl+C to stop watching once it's ready.

### Step 9: Access the Tomcat Server URL
Once the external IP is available, open a web browser (on your VM or elsewhere) and navigate to:
```
http://<external-ip>
```
Replace `<external-ip>` with the IP from Step 8. This should load the default Apache Tomcat welcome page.


Make sure to include port 8080














Terraform configure in aws

1. create a ec2 instance named terra-server.
2. create iam user - adminaccess
3. aws configure
install terraform - https://developer.hashicorp.com/terraform/install
sudo yum install -y yum-utils shadow-utils
sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
sudo yum -y install terraform

Now go to -> https://registry.terraform.io/providers/hashicorp/aws/latest/docs -> Authentication and Configuration 
create vim provider.tf in directory and paste:
provider "aws" {
  region     = "us-west-2"
  access_key = "my-access-key"
  secret_key = "my-secret-key"
}

Now go to (ec2 instance registry terraform -> https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/instance 
in vim provider.tf paste:
resource "aws_instance" "example" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = "t3.micro"

  tags = {
    Name = "HelloWorld"
  }
}

Now in place of ami paste aws ami catalog and copy id (till 64 bit) and paste
terraform validate
terraform init
terraform fmt
terraform plan
terraform apply






Making an instance with an existing security group through terraform

provider "aws" {
  region = "us-east-1"  # Replace with your desired region
}

# Reference an existing security group
data "aws_security_group" "existing_sg" {
  name = "existing-security-group-name"  # Replace with the actual security group name
  # Alternatively, use ID:
  # id = "sg-1234567890abcdef0"
}

# Launch two EC2 instances
resource "aws_instance" "my_instance" {
  count         = 2
  ami           = "ami-0c55b159cbfafe1f0"  # Replace with a valid AMI for your region
  instance_type = "t2.micro"
  security_groups = [data.aws_security_group.existing_sg.name]

  tags = {
    Name = "instance-${count.index + 1}"
  }
}

output "instance_ids" {
  value = aws_instance.my_instance[*].id
}





Making an instance with a new security group through terraform

1. Make an instance and install terraform there and make a directory.
2. Make a tf file:

provider "aws" {
  region = "us-east-1"  # Replace with your desired region
}

# Create a new security group
resource "aws_security_group" "my_sg" {
  name        = "my-security-group"
  description = "Security group for EC2 instances"

  # Ingress rule for SSH
  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]  # Allow SSH from anywhere (restrict for production)
  }

  # Ingress rule for HTTP (example)
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Egress rule (allow all outbound traffic)
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "my-security-group"
  }
}

# Launch two EC2 instances
resource "aws_instance" "my_instance" {
  count         = 2
  ami           = "ami-0e86e20dae9224db8"  # Valid AMI for us-east-1 (Amazon Linux 2, as of 2025)
  instance_type = "t2.micro"
  security_groups = [aws_security_group.my_sg.name]

  tags = {
    Name = "instance-${count.index + 1}"
  }
}

output "instance_ids" {
  value = aws_instance.my_instance[*].id
}








Making 2 instances with a new key pair login and existing security group


provider "aws" {
  region = "us-east-1"  # Replace with your desired region
}

# Reference an existing security group
data "aws_security_group" "existing_sg" {
  name = "existing-security-group-name"  # Replace with the actual security group name
  # Alternatively, use ID:
  # id = "sg-1234567890abcdef0"
}

# Create a new key pair
resource "aws_key_pair" "my_key_pair" {
  key_name   = "my-terraform-key"  # Name for the key pair in AWS
  public_key = tls_private_key.my_key.public_key_openssh
}

# Generate a new private key
resource "tls_private_key" "my_key" {
  algorithm = "RSA"
  rsa_bits  = 4096
}

# Save the private key to a local file
resource "local_file" "private_key" {
  content         = tls_private_key.my_key.private_key_pem
  filename        = "${path.module}/my-terraform-key.pem"
  file_permission = "0600"  # Secure permissions for the private key
}

# Launch two EC2 instances
resource "aws_instance" "my_instance" {
  count         = 2
  ami           = "ami-0c55b159cbfafe1f0"  # Replace with a valid AMI for your region
  instance_type = "t2.micro"
  key_name      = aws_key_pair.my_key_pair.key_name  # Associate the key pair
  security_groups = [data.aws_security_group.existing_sg.name]

  tags = {
    Name = "instance-${count.index + 1}"
  }
}

output "instance_ids" {
  value = aws_instance.my_instance[*].id
}

output "key_pair_name" {
  value = aws_key_pair.my_key_pair.key_name
}

output "private_key_path" {
  value = local_file.private_key.filename
}











Creating a custom VPC with terraform

Creating users in IAM using terraform

Creating variable VPC in teraform










Nginx image on k8s cluster and it should be available on port 80

1. subnet-068c6ca919d094394
subnet-0b33844693b77ca44

After eks cluster setup in ec2 instance,
Make a dir -> vim pypod.yaml

 

kubectl apply -f mypod.yaml
kubectl expose pod nginx --type=LoadBalancer --port=80
kubectl get svc
kubectl get pods












Git hub actions

CI-CD workflow using github actions for aws s3

reference link - https://medium.com/@bryant.logan1/ci-cd-workflow-using-github-actions-for-aws-s3-6e15bae5b2d0

1. Jenkins is self-hosted, so more secure. not gh actions; managed by micrososft. 
2. creat s3 bucket - gave name 'arka-github-actions-bucket' -> unblock all public access 
s3bucket -> properties -> static web hosting -> enable -> static -> index.html
Now bucket -> permissions -> bucket policy ->edit -> (give your bucket name here )
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": [
                "s3:GetObject"
            ],
            "Resource": [
                "arn:aws:s3:::Bucket-Name/*"
            ]
        }
    ]
}
3. create a GitHub repo : git@github.com:arkaprovolti/static-github-action-web-hosting-s3bucket.git
4. Now that the bucket is properly set up, you can head over to the IAM console to create a user that GitHub will use to upload files to S3.
5. 	Create user and give s3 full access -> create access key
Now to go to secrets and variables of our created GitHub repo -> actions -> new repository secret -> create 3 screts->
Now we’re going to add 3 secrets for the AWS_S3_BUCKET, AWS_ACCESS_KEY_ID, and AWS_SECRET_ACCESS_KEY

AWS_S3_BUCKET -> bucket name

now go to actions from our repo  -> set up a workflow yourself -> 

name: Upload Website

on:
  push:
    branches:
    - main

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@master
    - uses: jakejarvis/s3-sync-action@master
      with:
        args: --follow-symlinks --delete --exclude '.git*/*'
      env:
        SOURCE_DIR: ./
        AWS_REGION: us-east-1
        AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}



create index.html in repo



















S3 bucket access cross region

Make 2 buckets each in North Virginia and Ohio. Make sure enabling aws versioning on both sides.
Upload some objects on both buckets.
Now take one bucket -> Management -> Replication Rules -> Destination -> if u r in North Virginia, select Ohio ->Yes -> 
Now see bucket in ohio one





S3 bucket lifecycle configuration
Get into any bucket -> management -> Create lifecycle rule ->apply for all -> Lifecycle rule actions: Transition current versions of objects between storage classes ->day30 -standard 1a -> day 60 - intelligent tiering -> Day 90 One zone 1A -> Day 180 - Glacier flexible retrieval



















Provision an EKS cluster (AWS) using Terraform

1. Go to  the terraform init directory
2. Follow this tutorial - https://developer.hashicorp.com/terraform/tutorials/kubernetes/eks
3. cd learn-terraform-provision-eks-cluster





Deploy an nginx container to a fargate cluster and expose it to internet.

1. Create ecs cluster
2. Create a task definition - > ECS -> task definition -> create new task definition -> launch type - fargate -> container 1 -> image- nginx:latest
3. Create a loadbalancer and target group -> EC2 -> Load balancer -> Target groups -> create target group -> target type - ip address -> protocol -http, port 80 ->

4. Now create load balancer -> application load balancer ->nginx-lb
5. Create ECS service -> select the cluster u created ->create service -> load balancing -> select your loadbalancer ->
DNS from load balancer click 

(Make sure the subnets in service should be same as the ones in availability zones in load balancer) (Can also access it without loadbalancer by getting intol cluster -> service -> task -> public ip:8080

sir told us to containerize his maven app in docker and push it to ecr and deploy it to a fargate cluster and expose it to internet...treat that as HW





Containerizing a maven repo and pushing it to ecr and then deploying it to fargate cluster

1. clone repo
2. install java* , maven, git
3. inside repo, run - mvn package
4. run 4 push commands from ecr in cloned repo
security group.
5. Then repeat all the steps of "Deploy an nginx container to a fargate cluster and expose it to internet".






replicaset and deploy it to internet, and saling it

1. create eks cluster
install metrics server for autoscaling
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
create a directory
make replicaset yaml file:
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-replicaset
  labels:
    app: nginx
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi

apply the replicaset -> kubectl apply -f nginx-replicaset.yaml

Verify:
- `kubectl get replicaset` (should show `web-app-replicaset` with 1/1 pods).
- `kubectl get pods` (should show 1 pod running).



Now we will expose the replicaset with a service
vim service.yaml

apiVersion: v1
kind: Service
metadata:
  name: web-app-service
spec:
  selector:
    app: nginx  # Matches the ReplicaSet’s selector
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: LoadBalancer  # Use NodePort for local clusters like Minikube


kubectl apply -f service.yaml
kubectl get svc -> url

Now Set Up Horizontal Pod Autoscaler (HPA)

vim hpa.yaml

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler                                                                                                        metadata:
  name: web-app-hpa
spec:
  scaleTargetRef:                                                                                                                        apiVersion: apps/v1                                                                                                                  kind: ReplicaSet  # Targeting ReplicaSet directly
    name: nginx-replicaset                                                                                                             minReplicas: 1                                                                                                                       maxReplicas: 10  # Maximum replicas to scale to
  metrics:                                                                                                                             - type: Resource                                                                                                                       resource:
      name: cpu                                                                                                                            target:                                                                                                                                type: Utilization
        averageUtilization: 50  # Scale up if average CPU > 50%                                                                      





kubectl apply -f hpa.yaml
kubectl get hpa

Generate load to test scaling (e.g., using `hey` or `ab`):


Verify replicas: `kubectl get replicaset

 yum install httpd-tools

ab -n 10000 -c 100 http://abf1d0b25c998443ebbda26a4adade88-1322233573.us-east-1.elb.amazonaws.com/













Ansible - setup httpd server and copy fstab from worker node to remote server

setup ansible in 3 machines - 2 workers and 1 master

Now in control pane ->

---
- name: Fetch fstab from worker1
  hosts: worker1
  become: yes
  tasks:
    - name: Fetch fstab file from worker1
      fetch:
        src: /etc/fstab
        dest: /tmp/fstab_worker1
        flat: yes
      become: yes

- name: Copy fstab to worker2
  hosts: worker2
  become: yes
  tasks:
    - name: Ensure destination directory exists on worker2
      file:
        path: /backup/fstab_folder
        state: directory
        mode: '0755'
      become: yes

    - name: Copy fstab file to worker2
      copy:
        src: /tmp/fstab_worker1
        dest: /backup/fstab_folder/fstab
        mode: '0644'
      become: yes

    - name: Clean up temporary file on control node
      delegate_to: localhost
      file:
        path: /tmp/fstab_worker1
        state: absent


Now check in worker2, /backup


















Ansible get users

Cafter ansible setup in 3 machines, copy user code from sir's ansible repo -> Chapter 5 - Loops in Ansible/users-loop.yml -> users will be created in host machines
Now to check users -> ssh worker1 'cut -d: -f1 /etc/passwd | grep -E "sanjaya|drcula|iraonman"'
ssh worker2 'cut -d: -f1 /etc/passwd | grep -E "sanjaya|drcula|iraonman"'









Creating ec2 instance using ansible

1. Setup ansible with 1 master only
2. yum install unzip -y
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
create iam user
aws configure
yum install python3
yum install pip -y
#install pip install botocore
#install pip install boto3

aws ec2 describe-regions

Create inventory.ini ->

[localhost]
localhost ansible_connection=local ansible_python_interpreter=/usr/bin/python3

create creds.yaml

---
aws_access_key: YOUR_AWS_ACCESS_KEY_ID
aws_secret_key: YOUR_AWS_SECRET_ACCESS_KEY



create create-aws-ec2.yaml ->

---
- name: creating ec2 instance
  hosts: localhost
  vars_files:
    - creds.yaml
  tasks:
    - name: creating Ec2 instance via Ansible
      amazon.aws.ec2_instance:
        name: k8s-master
        instance_type: t3.micro
        image_id: ami-00ca32bbc84273381
        vpc_subnet_id: subnet-00f9c0eb863560b7c
        security_group: sg-0bb7891bd30c0f817
        key_name: 9sepkeya
        region: us-east-1
        count: 1
        state: present


ansible-playbook create-aws-ec2.yaml -i inventory.ini





in case of custom vpc using terraform, use new custom vpc yaml file from sir's terracode repo. Then run ssh-keygen on terraform machine and copy public key and paste it in key pair in yaml file for vpc code. Now db-server and web-server will be created. web-server can be accessed in terraform machine. For running db server, copy pvt key from terra machine, run web server in it and vim <keyname> -> paste private key. then run -> chmod 600 <key-name> -> Then run dn-server on web-server which in itself running in terra machine. When db machine accessed, try accessing google by ping google. It would show outgoing network but not ingoing.

